---
---
@article{guha2023alignment,
  title        = {AI Regulation Has Its Own Alignment Problem: The Technical and Institutional Feasibility of Disclosure, Registration, Licensing, and Auditing},
  author       = {Guha, Neel and Lawrence, Christie M. and  Gailmard, Lindsey A.  and Rodolfa, Kit T.  and Surani, Faiz and Raji, Rishi Bommasani, Inioluwa Deborah and Cuéllar, Mariano-Florentino  and Honigsberg, Colleen and Liang, Percy and Ho, Daniel E.},
  year         = {2023},
  abbr         = {Geo. Wash. L. Rev.},
  journal = {George Washington Law Review (forthcoming)},
  abstract = {Calls for regulating artificial intelligence (AI) are widespread, but there remains little consensus on both the specific harms that regulation can and should address and the appropriate regulatory actions to take. Computer scientists propose technical solutions that may be infeasible or illegal; lawyers propose regulation that may be technically impossible; and commentators propose policies that may backfire. AI regulation, in that sense, has its own alignment problem, where proposed interventions are often misaligned with societal values. In this Essay, we detail and assess the alignment and technical and institutional feasibility of four dominant proposals for AI regulation in the United States: disclosure, registration, licensing, and auditing. Our caution against the rush to heavily regulate AI without addressing regulatory alignment is underpinned by three arguments. First, AI regulatory proposals tend to suffer from both regulatory mismatch (i.e., vertical misalignment) and value conflict (i.e., horizontal misalignment). Clarity about a proposal’s objectives, feasibility, and impact may highlight that the proposal is mismatched with the harm intended to address. In fact, the impulse for AI regulation may in some instances be better addressed by non-AI regulatory reform. And the more concrete the proposed regulation, the more it will expose tensions and tradeoffs between different regulatory objectives and values. Proposals that purportedly address all that ails AI (safety, trustworthiness, bias, accuracy, and privacy) ignore the reality that many goals cannot be jointly satisfied. Second, the dominant AI regulatory proposals face common technical and institutional feasibility challenges—who in government should coordinate and enforce regulation, how can the scope of regulatory interventions avoid ballooning, and what standards and metrics operationalize trustworthy AI values given the lack of, and unclear path to achieve, technical consensus? Third, the federal government can, to varying degrees, reduce AI regulatory misalignment by designing interventions to account for feasibility and alignment considerations. We thus close with concrete recommendations to minimize misalignment in AI regulation.},
  pdf = {https://dho.stanford.edu/wp-content/uploads/AI_Regulation.pdf},
  blog = {https://hai.stanford.edu/policy-brief-ai-regulatory-alignment-problem}
}
@inproceedings{mello2023chatgpt,
  title        = {ChatGPT and Physicians’ Malpractice Risk},
  author       = {Mello, Michelle M. and Guha, Neel},
  booktitle    = {JAMA Health Forum},
  volume       = {4},
  number       = {5},
  pages        = {e231938--e231938},
  year         = {2023},
  organization = {American Medical Association},
  abbr         = {JAMA Forum},
  website      = {https://jamanetwork.com/journals/jama-health-forum/fullarticle/2805334},
  abstract     = {ChatGPT has exploded into the national consciousness. The potential for large language models (LLMs) such as ChatGPT, Bard, and many others to support or replace humans in a range of areas is now clear—and medical decisions are no exception. This has sharpened a perennial medicolegal question: How can physicians incorporate promising new technologies into their practice without increasing liability risk?}
}

@article{zambrano2023private,
  title    = {Private Enforcement in the States},
  author   = {Zambrano, Diego and Guha, Neel and Peters, Austin and Xia, Jeffrey},
  journal  = {University of Pennsylvania Law Review, forthcoming},
  year     = {2023},
  ssrn     = {4365144},
  abbr     = {U. Pa. L. Rev.},
  abstract = {Scholarship on U.S. litigation and civil procedure has scarcely studied the role of private enforcement in the states. Over the past two decades, scholars have established that, almost uniquely in the world, the U.S. often relies on private parties rather than administrative agencies to enforce important statutory provisions. Take your pick of any area in American governance and you will find private rights of action: environmental law, civil rights, employment discrimination, antitrust, consumer protection, business competition, securities fraud, and so on. In each of these areas, Congress deliberately empowered private plaintiffs instead of, or in addition to, government agencies. Yet, despite the vast importance of private enforcement at the federal level, we have no account of how prevalent private rights of action are in state law. And this question is particularly pressing now that a range of states—triggered by the Texas abortion law S.B.8.—are using private enforcement to weaken constitutional rights. Is private enforcement a way of governance in the states or just at the federal level? If it exists, are there important differences? What political conditions lead to their adoption? And why does it exist? <br><br>
              
              In this Article, we conduct the first systematic empirical investigation of the hidden world of state private enforcement. Using computational-linguistics and machine learning, we identify private enforcement provisions across a unique dataset of all states’ laws going back to 2000 (for all 50 states). Our results show that private enforcement is ubiquitous at the state level. Even by conservative estimates, there are more than 3,500 private rights of action provisions in state law, ranging from traditional areas like antitrust and employment, all the way to privacy violations, lawsuits against police, grave-digging, veterinary care, and waste disposal. Counterintuitively, private enforcement provisions are expanding the most in states like Utah, New Hampshire, Connecticut, Nebraska, and Wisconsin. Much of the growth in private enforcement is concentrated in areas affecting businesses, labor, the environment, and taxes. One takeaway from these results is that state private enforcement is strikingly different than the federal system—sprawling, messier, and even chaotic.
              <br><br>
              
              We also use our data to test conventional theories of private enforcement adoption. The most prominent one—the separation of powers theory—posits that Congress enacts private rights of action when the executive is controlled by another political party. Our empirical bottom line, based on regression analyses, is that we fail to find evidence in favor of any of the theories, including separation of powers. We even find no correlation between an increased adoption of private enforcement and legislative control by Democrats or Republicans. It appears the political economy of private enforcement in the states diverges radically from the federal government. With an eye toward future theorizing and empirical testing, we put forth three institutional differences between the states and federal government that may explain this divergence. And we sketch a future comparative research agenda focused on studying federal-state divergence. Reaffirming the central role that private enforcement plays in our system reveals the need to reorient civil procedure and incorporate state private rights of action more explicitly into its core teachings.}
}


@article{guha2022vulnerabilities,
  title     = {Vulnerabilities in Discovery Tech},
  author    = {Guha, Neel and Henderson, Peter and Zambrano, Diego},
  journal   = {Harvard Journal of Law \& Technology},
  volume    = {35},
  number    = {2},
  year      = {2022},
  publisher = {HeinOnline},
  abbr      = {JOLT},
  abstract  = {Recent technological advances are changing the litigation landscape, especially in the context of discovery. For nearly two decades, technologies have reinvented document searches in complex litigation, normalizing the use of machine learning algorithms under the umbrella of “Technology Assisted Review” (TAR). But the latest technological developments are placing discovery beyond the reach of attorney understanding and firmly in the realm of computer science and engineering. As lawyers struggle to keep up, a creeping sense of anxiety is spreading in the legal profession about a lack of transparency and the potential for discovery abuse. Judges, attorneys, bar associations, and scholars warn that lawyers need to closely supervise the technical aspects of TAR and avoid the dangers of sabotage, intentional hacking, or abuse. But these commentators have not fully defined with precision what the risks entail, furnished a clear outline of potential dangers, or defined the appropriate boundaries of debate.
               <br><br>
               This Article provides a systematic assessment of the potential for abuse in technology-assisted discovery. The Article offers three contributions. First, our most basic aim is to provide a technical but accessible assessment of vulnerabilities in the typical TAR process. To do so, we use the latest computer science research to identify and catalogue the different ways that TAR can go awry, either due to intentional abuse or mistakes. Second, with a better understanding of how discovery can be subverted, we then map potential remedies and reassess current debates in a more helpful light. The upshot is that abuse of technology-assisted discovery is possible but can be preventable if the right review processes are in place. Finally, we propose reforms to improve the system in the short and long term, with an emphasis on improved metrics that can more fully measure the quality of TAR. By exploring the technical background of discovery abuse, the Article demystifies the engineering substrate of modern discovery. Undertaking this study shows that with the right technical knowledge and assistance, lawyers can safeguard technology- assisted discovery without surrendering professional jurisdiction.},
  ssrn      = {4065997},
  pdf       = {https://jolt.law.harvard.edu/assets/articlePDFs/v35/4.-Guha-Henderson-and-Zambrano-Vulnerabilities-in-Discovery-Tech.pdf}
}

@misc{nrc,
  author  = {Daniel E. Ho and Jennifer King and Russell C. Wald and Christopher Wan},
  note    = {Contributing Author: §5 (Data Privacy Compliance), §6 (Technical Privacy and Virtual Data Safe Rooms), §8 (Managing Cybersecurity Risks).},
  title   = {Building a National AI Research Resource: A Blueprint for the National Research Cloud},
  year    = {2021},
  abbr    = {White Paper},
  pdf     = {https://hai.stanford.edu/sites/default/files/2022-01/HAI_NRCR_v17.pdf},
  website = {https://hai.stanford.edu/white-paper-building-national-ai-research-resource}
}